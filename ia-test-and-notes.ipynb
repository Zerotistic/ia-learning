{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "end of creation of training and testing dataset\n",
      "Predictions: [ 87112.22868401 311287.67895144 149138.7569972  181605.04417126\n",
      " 242876.06079414]\n",
      "Labels: [72100.0, 279600.0, 82700.0, 112500.0, 238300.0]\n",
      "68740.3505914331\n",
      "0.0\n",
      "Scores:  [71441.95984286 68976.04689016 68122.48637398 71887.89226925\n",
      " 66217.48472738 75775.82777537 69522.97407568 70146.52009504\n",
      " 69247.16265638 70498.64662739]\n",
      "Mean:  70183.70013334931\n",
      "Standard deviation:  2420.9813729122807\n",
      "Scores:  [72195.7979865  64520.15075203 67826.0279554  69089.72827829\n",
      " 66747.91999171 72801.81116412 70339.52226055 69246.21959031\n",
      " 66692.04641613 70356.66638109]\n",
      "Mean:  68981.58907761266\n",
      "Standard deviation:  2453.8299826853217\n",
      "Scores:  [51425.21675222 49015.10991141 46580.23163609 51780.84641095\n",
      " 47446.09614329 51686.62946029 52380.61643189 49618.42736031\n",
      " 48750.51270029 53751.16987483]\n",
      "Mean:  50243.4856681574\n",
      "Standard deviation:  2192.8793454374454\n"
     ]
    }
   ],
   "source": [
    "from ast import fix_missing_locations\n",
    "import os \n",
    "import tarfile\n",
    "import urllib\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zlib import crc32\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "\n",
    "DL_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\" # root for datasets\n",
    "H_PATH = os.path.join(\"datasets\", \"housing\") \n",
    "H_URL = DL_ROOT + \"datasets/housing/housing.tgz\" # full url for the housing csv\n",
    "\n",
    "\n",
    "# when calling fetch_housing_data(), it creates a `datasets/housing` directory, downloads the tar file and extract housing.csv file in the dir\n",
    "def fetch_housing_data(housing_url = H_URL, housing_path=H_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")    \n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "# load csv file\n",
    "def load_housing_data(housing_path=H_PATH):\n",
    "    csv_path = os.path.join(housing_path ,\"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "fetch_housing_data()\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#                                    DATA EXPLORATION                                   #\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "# shows top five rowq usisng DataFrame's head() method\n",
    "housing.head()\n",
    "# the info() method is useful to get a quick desc of the data. in particular the total number of rows, each attribute's type, and the number of nonnull values\n",
    "housing.info()\n",
    "# value_counts() counts the number of value who have a specific label\n",
    "housing[\"ocean_proximity\"].value_counts()\n",
    "# describe() method shows a summary of the numeriacal attributes\n",
    "housing.describe()\n",
    "#\n",
    "# it also exists count(), mean(), min(), max(). They are self-explanatory \n",
    "#\n",
    "#\n",
    "# TO MAKES SOME FANCY GRAPH:\n",
    "# \n",
    "# hist() method works by using matplotlib, which do some magic that needs the first line to be `*matplotlib inline`\n",
    "# \n",
    "# \n",
    "# %matplotlib inline # note that this works only in jupyter notebook\n",
    "# import matplotlib.pyplot as plt\n",
    "# housing.hist(bins=50, figsize=(20,15))\n",
    "# plt.show() \n",
    "\n",
    "\n",
    "# we have to create two dataset: testing and training. here is an implementation:\n",
    "###### NOTE\n",
    "# this is not the best implementation, because of risk in case of refresh of datasets, and restart of computing for the ML \n",
    "# there's a better implementation for that using hash below this commented-code section  \n",
    "#\n",
    "# def split_train_test(data, test_ratio):\n",
    "#     # shuffle datasets\n",
    "#     shuffled_indices = np.random.permutation(len(data))\n",
    "#     test_set_size = int(len(data)*test_ratio)\n",
    "#     # takes a test dataset\n",
    "#     test_indices = shuffled_indices[:test_set_size]\n",
    "#     # takes a training dataset\n",
    "#     train_indices = shuffled_indices[test_set_size:]\n",
    "#     return data.iloc[train_indices], data.iloc[test_indices]\n",
    "#\n",
    "# create test & training dataset\n",
    "# \n",
    "# train_set, test_set = split_train_test(housing, 0.2)\n",
    "# print(len(train_set))\n",
    "# print(len(test_set))\n",
    "\n",
    "# use each instance's identifier to decide wether or not it should go in the test set (assuming instances have a unique and immutable identifier)\n",
    "# compute a hash of each instance's identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value\n",
    "# this ensure that the test set will remain consistent across multiple runs, even if you refresh the dataset\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "# \n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "housing_with_id = housing.reset_index() # adds and `index` column\n",
    "# build a unique identifier. here : district's latitude and longitude are guaranteed to be stable for a few million years\n",
    "# so i combine them to create an ID like so: \n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "print(\"end of creation of training and testing dataset\")\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                              bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                              labels=[1,2,3,4,5])\n",
    "\n",
    "housing[\"income_cat\"].hist()\n",
    "\n",
    "# stratified sampling based on the income category. we're using Scikit-Learn's `StratifiedShuffleSplit`:\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "# verify if it worked: here we're looking at the income category proportions in the test set\n",
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "\n",
    "# TO ONLY \n",
    "\n",
    "# because i like fancy graphic, here's how to do: \n",
    "# (using pandas for that)\n",
    "# kind allows to choose different plot type (for more info https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.plot.html)\n",
    "# x and y are the label for the x and y axis\n",
    "# alpha=0.1 makes it much easier to visualize the places where there's a high density of data points\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "\n",
    "# for a more precise / informative plot:\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1,\n",
    "             s=housing[\"population\"]/100, label=\"population\",figsize=(10,7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "             )\n",
    "# show the graphs\n",
    "plt.legend()\n",
    "\n",
    "# compute the Standard Correlation Coefficient (also called Pearson's r) between every paire of attributes using the corrr() method\n",
    "# this works because the dataset is not too large\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "# another method to check for correlation between attributes is to use the pandas `scatter_matrix()` function, which plots every numerical against every other\n",
    "# numerical attribute\n",
    "attributes = [\"median_house_value\", \"median_income\",  \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#                                      DATA CLEANING                                    #\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#########################################################################################\n",
    "\n",
    "# clear training set (by copying strat_train_set)\n",
    "# separate the predictors and the labels, since we don't necessarily want to apply the same transformations to the predictors and the target values\n",
    "# note that here, `drop()` creates a copy of the data and does not affect strat_train_set\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "housing.dropna(subset=[\"total_bedrooms\"])\n",
    "\n",
    "# Scikit-Learn provides a handy class to take care of missing values: SimpleImputer\n",
    "# First, you need to create a SimpleImputer() instance, specifying that you want to replace each attribute's missing values with the median of that attribute\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "# since the median can only be computed on numerical attributes, you need yo create a copy of the data without the text attribute ocean_proximity\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "# Now you can fit the imputer instance to the training data using the fit() method\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# text attributes\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "# shows it's 10 first attributes\n",
    "housing_cat.head(10)\n",
    "# this is a categorical attribute, so we need to convert it to numbers\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]\n",
    "# you can get the list of categories by using the categories_ instance variable\n",
    "# it's a list containing a 1D array of categories for each categorical attribute\n",
    "ordinal_encoder.categories_ \n",
    "# create a binary attribute per category: one attribute equal to 1 when the category is \"<1H OCEAN\" (and 0 otherwise), and so on.\n",
    "# This is called one-hot encoding, because only one attribute will be equal to one\n",
    "# The new attributes are sometimes called dummy attributes. Scikit-Learn provides a `OneHotEncoder()` class to convert categorical values into onhot vectors\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot.toarray()\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#                                  CUSTOM TRANSFORMERS                                  #\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#########################################################################################\n",
    "\n",
    "# you may need to wrute your own transformers for tasks such as custom cleanup operations or combining specific attributes\n",
    "# all you need to do is create a class and implement three methods: fit(), transform() and fit_transform()\n",
    "# you can get the last one for free simply by adding TransformerMixin as a base class\n",
    "# if you add BaseEstimator as a base class (and avoid *args and **kargs in your constructor), you will also get two extra methods (get_params() and set_params())\n",
    "# that will be useful for automatic hyperparameter tuning\n",
    "#\n",
    "# The following is a small transformer class that adds the combined attributes\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, population_ix] / X[:, households_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#                               TRANSFORMATION PIPELINE                                 #\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "# There are many data transformation steps that need to be executed in the right order\n",
    "# Scikit-Learn provides the Pipeline class to help with such sequences of transformatioins\n",
    "# The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps.\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                        ('attribs_adder', CombinedAttributesAdder()),\n",
    "                        ('std_scaler', StandardScaler()),\n",
    "                        ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "# We get the list of numerical column names and the list of categorical column names, and then we construct ColumnTransformer\n",
    "# The constructor requires a list of tuples, where each tuple contains a name, a transformer, and a list of names (or indices) of columns that the transformer\n",
    "# should be applied to.\n",
    "# we specify that the numerical columns should be transformed using the num_pipeline that we defined earlier, and the categorical columns should be transformed\n",
    "# using a OneHotEncoder\n",
    "# Finally we apply this ColumnTransformer to the housing data: it applies each transformer to the appropriate columns and concatenates the outputs along the \n",
    "# second axis\n",
    "##### NOTE\n",
    "# OneHotEncoder returns a sparse matrix, while the num_pipeline returns a dense matrix. \n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_attribs),\n",
    "                                  (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "                                  ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#                              SELECT AND TRAIN A MODEL                                 #\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "# The first (and probably easier one) is the Linear Regression\n",
    "# Let's first train a Linear Regression model, like we did previously\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "# Done! We now have a Linear Regression model\n",
    "# Let's try it out on a few instances from the training set\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))\n",
    "# It works, although the predictions are not exactly accurate\n",
    "# Let's measure this regression model's RMSE on the whole training set using Scikit-Learn's mean_squared_error()\n",
    "# What is RMSE ?\n",
    "# RMSE is a performance measure. It stands for Root Mean Square Error. It gives and idea of how much error the system typically makes in its prediction\n",
    "# with a higher weight for large errors.\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "\n",
    "# This is better than nothing, but clearly not a great score: most districts' median_housing_values range between $120,000 abd $256,000\n",
    "# so a typical error prediction error of ~$68,000 is not very satisfying. This model is underfitting the training data\n",
    "# This either means that we do not provides enough data, or that the model is not powerful enough\n",
    "# We can fix underfitting by: selecting a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model\n",
    "# Let's try a more complex model!\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "# Now that it's trained, let's evaluate it on the training set:\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(tree_rmse)\n",
    "\n",
    "# Whaaaaaat?! No error at all? So this model is absolutely perfect? It's possible that the model has badly ofervit the data.\n",
    "# How can you be sure?\n",
    "# You need to not touch the test dataset until you are ready to launch a model you're confident about, so you need to use part of the training set for training\n",
    "\n",
    "# Introducing Cross-Validation\n",
    "# One way to evaluate the Decision Tree model would be to use the train_test_split() function to split the training set into smaller training set and \n",
    "# a validation set. \n",
    "# It requires a little bit of work, but nothing too difficult\n",
    "# \n",
    "# A great alternative is to use Scikit-Learn's K-fold cross-validation feature\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "# now let's look at the results:\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Standard deviation: \", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)\n",
    "\n",
    "# Now it doesn't look as good as it did earlier. In fact it seems to perform worse than the Linear Regression\n",
    "# let's compute the same for the Linear Regression\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)\n",
    "\n",
    "# Let's try one last model: RandomForestRegressor\n",
    "# Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions\n",
    "# Building a model on top of other models is called Ensemble Learning, and it is often a great way to push ML algorithms even further\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n",
    "\n",
    "# That's better, however the model is still overfitting\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#                                  FINE TUNE YOUR MODEL                                 #\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#########################################################################################\n",
    "\n",
    "# You should use Scikit-Learn's GridSearchCV to search for you the best params for your model.\n",
    "\n",
    "param_grid = [{'n_estimators':[3,10,30],'max_features':[2,4,6,8]},\n",
    "              {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2,3,4]},\n",
    "              ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "# show the best params\n",
    "grid_search.best_params_\n",
    "# {'max_features': 8, 'n_estimators': 30}\n",
    "grid_search.best_estimator_\n",
    "# RandomForestRegressor(max_features=8, n_estimators=30)\n",
    "#The evalutation score are available in case you want to see all of them\n",
    "#\n",
    "# cvres = grid_search.cv_results_\n",
    "# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "#     print(np.sqrt(-mean_score),params)\n",
    "#\n",
    "# Here we obtain the best solution by setting max_features hyperparameter to 8 and n_estimators to 30\n",
    "\n",
    "# You could also use RandomizedSearchCV instead of GridSearchCV\n",
    "# it has some benefits when the hyperparameter search soace is large\n",
    "\n",
    "# Let's analyze the best models and their errors\n",
    "# to do so, we need to display the importance scores next to their corresponding attribute names:\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encode = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attribtues = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n",
    "\n",
    "# With those info we now know we could drop some less useful features\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#                                  FINE TUNE YOUR MODEL                                 #\n",
    "#                                                                                       #\n",
    "#                                                                                       #\n",
    "#########################################################################################\n",
    "\n",
    "# We're now going to evaluate the system on the test set\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "display_scores(final_rmse)\n",
    "\n",
    "# Although it is better, how can you be sure of that?\n",
    "# well you can compute a 95% confidence interval for the generalization error using scipy.stats.t.interval() \n",
    "\n",
    "confidence = 0.95\n",
    "squared_errors = (final_predictions - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors)-1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))\n",
    "\n",
    "# now that the model is done and you can use it, just save it with joblib\n",
    "\n",
    "joblib.dump(final_model, \"my_model.pkl\") # final_model is my fine-tuned model\n",
    "# you can then load it later on by doing the following\n",
    "my_model_loaded = joblib.load(\"my_model.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103dfd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
